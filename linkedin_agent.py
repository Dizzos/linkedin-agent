import anthropic
import os
import json
import requests
from typing import Dict, Any, List
from datetime import datetime, timedelta
import feedparser
from collections import Counter

class LinkedInAgent:
    """
    LinkedIn –∞–≥–µ–Ω—Ç —Å Claude –∏ –ë–ï–°–ü–õ–ê–¢–ù–´–ú –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º —Ç—Ä–µ–Ω–¥–æ–≤
    """
    
    def __init__(self, anthropic_api_key: str, linkedin_access_token: str, 
                 industry: str = "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", target_audience: str = ""):
        self.client = anthropic.Anthropic(api_key=anthropic_api_key)
        self.linkedin_token = linkedin_access_token
        self.conversation_history = []
        self.industry = industry
        self.target_audience = target_audience
        
        # RSS —Ñ–∏–¥—ã –ø–æ –∏–Ω–¥—É—Å—Ç—Ä–∏—è–º (–±–µ—Å–ø–ª–∞—Ç–Ω–æ!)
        self.rss_feeds = {
            "product_management": [
                "https://www.mindtheproduct.com/feed/",
                "https://medium.com/feed/@ProductCoalition",
                "https://www.producttalk.org/feed/",
                "https://www.intercom.com/blog/feed/",
                "https://www.lennysnewsletter.com/feed",
            ],
            "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": [
                "https://techcrunch.com/feed/",
                "https://www.theverge.com/rss/index.xml",
                "https://news.ycombinator.com/rss",
                "https://arstechnica.com/feed/",
            ],
            "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥": [
                "https://www.searchenginejournal.com/feed/",
                "https://moz.com/blog/feed",
                "https://contentmarketinginstitute.com/feed/",
            ],
            "startup": [
                "https://news.ycombinator.com/rss",
                "https://techcrunch.com/category/startups/feed/",
                "https://www.reddit.com/r/startups/.rss",
            ],
            "ai": [
                "https://news.ycombinator.com/rss",
                "https://www.reddit.com/r/artificial/.rss",
                "https://www.reddit.com/r/MachineLearning/.rss",
            ]
        }
        
        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ subreddits –¥–ª—è –ø—Ä–æ–¥–∞–∫—Ç –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
        self.product_subreddits = [
            "ProductManagement",
            "product_design", 
            "startups",
            "SaaS",
            "userexperience",
            "analytics"
        ]
        
        self.tools = [
            {
                "name": "create_linkedin_post",
                "description": "–°–æ–∑–¥–∞–µ—Ç –∏ –ø—É–±–ª–∏–∫—É–µ—Ç –ø–æ—Å—Ç –≤ LinkedIn",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "content": {"type": "string"},
                        "visibility": {
                            "type": "string",
                            "enum": ["PUBLIC", "CONNECTIONS"],
                            "default": "PUBLIC"
                        }
                    },
                    "required": ["content"]
                }
            },
            {
                "name": "get_product_trends",
                "description": "–õ–£–ß–®–ò–ô –í–´–ë–û–† –¥–ª—è –ø—Ä–æ–¥–∞–∫—Ç –∞—É–¥–∏—Ç–æ—Ä–∏–∏! –ü–æ–ª—É—á–∞–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –∏–∑ –≤—Å–µ—Ö product-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: Mind the Product, r/ProductManagement, r/SaaS, r/startups, Hacker News. –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ –ø–µ—Ä–≤—ã–º –¥–µ–ª–æ–º!",
                "input_schema": {
                    "type": "object",
                    "properties": {}
                }
            },
            {
                "name": "web_search_trends",
                "description": "–ò—â–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫. –ë–ï–°–ü–õ–ê–¢–ù–û —á–µ—Ä–µ–∑ Claude web_search.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç—Ä–µ–Ω–¥–æ–≤"
                        }
                    },
                    "required": ["query"]
                }
            },
            {
                "name": "parse_rss_feeds",
                "description": "–ü–∞—Ä—Å–∏—Ç RSS —Ñ–∏–¥—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –ò—Å–ø–æ–ª—å–∑—É–π 'product_management' –¥–ª—è PM –∫–æ–Ω—Ç–µ–Ω—Ç–∞.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "industry": {
                            "type": "string",
                            "description": "–ò–Ω–¥—É—Å—Ç—Ä–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∏–¥–æ–≤: product_management, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, startup"
                        },
                        "limit": {
                            "type": "integer",
                            "description": "–ú–∞–∫—Å–∏–º—É–º –Ω–æ–≤–æ—Å—Ç–µ–π",
                            "default": 10
                        }
                    },
                    "required": ["industry"]
                }
            },
            {
                "name": "get_hackernews_trends",
                "description": "–ü–æ–ª—É—á–∞–µ—Ç trending —Ç–æ–ø–∏–∫–∏ —Å Hacker News. –ë–ï–°–ü–õ–ê–¢–ù–û —á–µ—Ä–µ–∑ API.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "limit": {
                            "type": "integer",
                            "description": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø–∏–∫–æ–≤",
                            "default": 10
                        }
                    }
                }
            },
            {
                "name": "get_reddit_trends",
                "description": "–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è —Å Reddit. –î–ª—è PM –∏—Å–ø–æ–ª—å–∑—É–π: ProductManagement, product_design, SaaS, startups",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "subreddit": {
                            "type": "string",
                            "description": "–ù–∞–∑–≤–∞–Ω–∏–µ subreddit (ProductManagement, product_design, SaaS, startups)"
                        },
                        "time_filter": {
                            "type": "string",
                            "enum": ["day", "week", "month"],
                            "default": "week"
                        },
                        "limit": {
                            "type": "integer",
                            "default": 10
                        }
                    },
                    "required": ["subreddit"]
                }
            },
            {
                "name": "analyze_trending_keywords",
                "description": "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "sources_data": {
                            "type": "string",
                            "description": "JSON —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
                        }
                    },
                    "required": ["sources_data"]
                }
            },
            {
                "name": "validate_topic_relevance",
                "description": "–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–º—ã –¥–ª—è –ø—Ä–æ–¥–∞–∫—Ç –∞—É–¥–∏—Ç–æ—Ä–∏–∏",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "topic": {"type": "string"},
                        "audience": {"type": "string"}
                    },
                    "required": ["topic"]
                }
            }
        ]
    
    def web_search_trends(self, query: str) -> Dict[str, Any]:
        """
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π web_search –æ—Ç Claude –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç—Ä–µ–Ω–¥–æ–≤
        –ë–ï–°–ü–õ–ê–¢–ù–û - —ç—Ç–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç Claude
        """
        return {
            "success": True,
            "query": query,
            "instruction": "use_claude_web_search",
            "message": f"–ò—Å–ø–æ–ª—å–∑—É–π –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π web_search –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}"
        }
    
    def parse_rss_feeds(self, industry: str, limit: int = 10) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏—Ç RSS —Ñ–∏–¥—ã - –ü–û–õ–ù–û–°–¢–¨–Æ –ë–ï–°–ü–õ–ê–¢–ù–û
        """
        feeds = self.rss_feeds.get(industry.lower(), self.rss_feeds.get("—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", []))
        
        all_articles = []
        
        for feed_url in feeds:
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries[:5]:
                    published = entry.get('published_parsed', None)
                    if published:
                        pub_date = datetime(*published[:6])
                    else:
                        pub_date = datetime.now()
                    
                    all_articles.append({
                        "title": entry.get('title', ''),
                        "link": entry.get('link', ''),
                        "summary": entry.get('summary', '')[:200],
                        "published": pub_date.strftime("%Y-%m-%d"),
                        "source": feed.feed.get('title', 'Unknown')
                    })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {feed_url}: {e}")
                continue
        
        all_articles.sort(key=lambda x: x['published'], reverse=True)
        
        return {
            "success": True,
            "industry": industry,
            "articles": all_articles[:limit],
            "total": len(all_articles)
        }
    
    def get_hackernews_trends(self, limit: int = 10) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ø–æ–≤—ã–µ —Ç–µ–º—ã —Å Hacker News - –ë–ï–°–ü–õ–ê–¢–ù–û
        """
        try:
            top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = requests.get(top_stories_url, timeout=10)
            response.raise_for_status()
            story_ids = response.json()[:limit]
            
            stories = []
            for story_id in story_ids:
                story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                story_response = requests.get(story_url, timeout=5)
                if story_response.ok:
                    story = story_response.json()
                    stories.append({
                        "title": story.get('title', ''),
                        "url": story.get('url', ''),
                        "score": story.get('score', 0),
                        "comments": story.get('descendants', 0)
                    })
            
            return {
                "success": True,
                "source": "Hacker News",
                "stories": stories,
                "total": len(stories)
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_reddit_trends(self, subreddit: str, time_filter: str = "week", 
                         limit: int = 10) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –ø–æ—Å—Ç—ã —Å Reddit - –ë–ï–°–ü–õ–ê–¢–ù–û
        """
        try:
            url = f"https://www.reddit.com/r/{subreddit}/top.json"
            params = {
                "t": time_filter,
                "limit": limit
            }
            headers = {
                "User-Agent": "LinkedInAgent/1.0"
            }
            
            response = requests.get(url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            posts = []
            for post in data['data']['children']:
                post_data = post['data']
                posts.append({
                    "title": post_data.get('title', ''),
                    "score": post_data.get('score', 0),
                    "comments": post_data.get('num_comments', 0),
                    "url": f"https://reddit.com{post_data.get('permalink', '')}",
                    "created": datetime.fromtimestamp(
                        post_data.get('created_utc', 0)
                    ).strftime("%Y-%m-%d")
                })
            
            return {
                "success": True,
                "subreddit": subreddit,
                "posts": posts,
                "total": len(posts)
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_product_trends(self) -> Dict[str, Any]:
        """
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è –ø—Ä–æ–¥–∞–∫—Ç –∞—É–¥–∏—Ç–æ—Ä–∏–∏
        """
        all_trends = {
            "rss_articles": [],
            "reddit_discussions": [],
            "hn_stories": []
        }
        
        # 1. Product RSS —Ñ–∏–¥—ã
        rss_result = self.parse_rss_feeds("product_management", limit=10)
        if rss_result.get("success"):
            all_trends["rss_articles"] = rss_result.get("articles", [])
        
        # 2. Product subreddits
        for subreddit in self.product_subreddits[:3]:
            reddit_result = self.get_reddit_trends(subreddit, "week", 5)
            if reddit_result.get("success"):
                for post in reddit_result.get("posts", []):
                    post["subreddit"] = subreddit
                    all_trends["reddit_discussions"].append(post)
        
        # 3. Hacker News
        hn_result = self.get_hackernews_trends(10)
        if hn_result.get("success"):
            all_trends["hn_stories"] = hn_result.get("stories", [])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords_result = self.analyze_trending_keywords(json.dumps(all_trends))
        
        return {
            "success": True,
            "sources": {
                "rss_count": len(all_trends["rss_articles"]),
                "reddit_count": len(all_trends["reddit_discussions"]),
                "hn_count": len(all_trends["hn_stories"])
            },
            "data": all_trends,
            "trending_keywords": keywords_result.get("top_keywords", []),
            "summary": "–î–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã –∏–∑ product-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
        }
    
    def analyze_trending_keywords(self, sources_data: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        try:
            data = json.loads(sources_data)
            
            all_text = []
            for source in data.values():
                if isinstance(source, list):
                    for item in source:
                        if isinstance(item, dict):
                            all_text.append(item.get('title', ''))
                            all_text.append(item.get('summary', ''))
            
            words = []
            for text in all_text:
                if text:
                    words.extend([
                        w.lower() for w in text.split() 
                        if len(w) > 4 and w.isalpha()
                    ])
            
            word_freq = Counter(words).most_common(10)
            
            return {
                "success": True,
                "top_keywords": [{"word": w, "count": c} for w, c in word_freq],
                "total_sources": len(data)
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def validate_topic_relevance(self, topic: str, audience: str = None) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–º—ã
        """
        if audience is None:
            audience = self.target_audience
        
        relevance_score = 75
        
        fresh_keywords = ["2025", "2024", "–Ω–æ–≤—ã–π", "latest", "breakthrough", "trend", "—Å–µ–≥–æ–¥–Ω—è"]
        if any(kw in topic.lower() for kw in fresh_keywords):
            relevance_score += 15
        
        if self.industry.lower() in topic.lower():
            relevance_score += 10
        
        is_relevant = relevance_score >= 70
        
        return {
            "success": True,
            "topic": topic,
            "relevance_score": min(relevance_score, 100),
            "is_relevant": is_relevant,
            "recommendation": "‚úÖ –ê–∫—Ç—É–∞–ª—å–Ω–æ" if is_relevant else "‚ùå –ù–∏–∑–∫–∞—è –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å"
        }
    
    def create_linkedin_post(self, content: str, visibility: str = "PUBLIC") -> Dict[str, Any]:
        """
        –ü—É–±–ª–∏–∫—É–µ—Ç –ø–æ—Å—Ç –≤ LinkedIn
        """
        user_info_url = "https://api.linkedin.com/v2/userinfo"
        headers = {
            "Authorization": f"Bearer {self.linkedin_token}",
            "Content-Type": "application/json"
        }
        
        try:
            user_response = requests.get(user_info_url, headers=headers)
            user_response.raise_for_status()
            user_data = user_response.json()
            user_id = user_data.get('sub')
            
            post_url = "https://api.linkedin.com/v2/ugcPosts"
            post_data = {
                "author": f"urn:li:person:{user_id}",
                "lifecycleState": "PUBLISHED",
                "specificContent": {
                    "com.linkedin.ugc.ShareContent": {
                        "shareCommentary": {"text": content},
                        "shareMediaCategory": "NONE"
                    }
                },
                "visibility": {
                    "com.linkedin.ugc.MemberNetworkVisibility": visibility
                }
            }
            
            response = requests.post(post_url, headers=headers, json=post_data)
            response.raise_for_status()
            
            return {
                "success": True,
                "post_id": response.json().get('id'),
                "message": "‚úÖ –ü–æ—Å—Ç —É—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω!"
            }
        except requests.exceptions.RequestException as e:
            return {
                "success": False,
                "error": str(e),
                "message": "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"
            }
    
    def process_tool_call(self, tool_name: str, tool_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—ã–∑–æ–≤—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        """
        tool_map = {
            "create_linkedin_post": self.create_linkedin_post,
            "get_product_trends": self.get_product_trends,
            "web_search_trends": self.web_search_trends,
            "parse_rss_feeds": self.parse_rss_feeds,
            "get_hackernews_trends": self.get_hackernews_trends,
            "get_reddit_trends": self.get_reddit_trends,
            "analyze_trending_keywords": self.analyze_trending_keywords,
            "validate_topic_relevance": self.validate_topic_relevance
        }
        
        if tool_name in tool_map:
            return tool_map[tool_name](**tool_input)
        return {"success": False, "error": f"Unknown tool: {tool_name}"}
    
    def chat(self, user_message: str) -> str:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç tool_use/tool_result –ø–∞—Ä—ã
    """
    self.conversation_history.append({
        "role": "user",
        "content": user_message
    })

    system_prompt = f"""–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π LinkedIn Content Manager –¥–ª—è –ü–†–û–î–ê–ö–¢ –ê–£–î–ò–¢–û–†–ò–ò —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º —Ç—Ä–µ–Ω–¥–æ–≤.

–ò–Ω–¥—É—Å—Ç—Ä–∏—è: {self.industry}
üéØ –¶–ï–õ–ï–í–ê–Ø –ê–£–î–ò–¢–û–†–ò–Ø: {self.target_audience or "Product Managers, Directors of Product, Product Leads"}
- 80% –ø—Ä–æ–¥–∞–∫—Ç –º–µ–Ω–µ–¥–∂–µ—Ä—ã, –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤, –ø—Ä–æ–¥–∞–∫—Ç –ª–∏–¥—ã
- Senior level –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—ã
- –ò–Ω—Ç–µ—Ä–µ—Å—É—é—Ç—Å—è: —Å—Ç—Ä–∞—Ç–µ–≥–∏—è, –º–µ—Ç—Ä–∏–∫–∏, frameworks, –∫–µ–π—Å—ã, –∫–æ–º–∞–Ω–¥–∞

üìä –ü–†–û–î–ê–ö–¢-–°–ü–ï–¶–ò–§–ò–ß–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò:
1. **parse_rss_feeds** —Å "product_management" - Mind the Product, Product Coalition, Intercom, Lenny's Newsletter
2. **get_reddit_trends** - r/ProductManagement, r/product_design, r/SaaS, r/startups
3. **get_hackernews_trends** - tech –∏ product –æ–±—Å—É–∂–¥–µ–Ω–∏—è
4. **web_search_trends** - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è

üî• –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ï –¢–ï–ú–´ –î–õ–Ø PM –ê–£–î–ò–¢–û–†–ò–ò:
- Product strategy & vision
- Product-market fit
- User research & discovery
- Roadmapping & prioritization
- Metrics & analytics (retention, engagement, NPS)
- Product-led growth
- Stakeholder management
- Team leadership
- AI in product development
- Framework'–∏ (RICE, Jobs-to-be-Done, etc.)

WORKFLOW:
1. –ú–æ–Ω–∏—Ç–æ—Ä—å product-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (Mind the Product, r/ProductManagement)
2. –ò—â–∏ —Ç–µ–º—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–ª—è PM (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è, –º–µ—Ç—Ä–∏–∫–∏, –∫–æ–º–∞–Ω–¥–∞)
3. –í–∞–ª–∏–¥–∏—Ä—É–π —á–µ—Ä–µ–∑ validate_topic_relevance (‚â•70%)
4. –°–æ–∑–¥–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π, actionable –∫–æ–Ω—Ç–µ–Ω—Ç
5. –ü—É–±–ª–∏–∫—É–π —á–µ—Ä–µ–∑ create_linkedin_post

üìù –°–¢–ò–õ–¨ –î–õ–Ø –ü–†–û–î–ê–ö–¢ –ê–£–î–ò–¢–û–†–ò–ò:
- **–¢–æ–Ω**: –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π, –Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–π. –ü—Ä–∞–∫—Ç–∏—á–Ω—ã–π, –±–µ–∑ –≤–æ–¥—ã
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∞**: 
  * –°–∏–ª—å–Ω—ã–π —Ö—É–∫ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π PM
  * –ö–æ–Ω—Ç–µ–∫—Å—Ç/–¥–∞–Ω–Ω—ã–µ (—Ü–∏—Ñ—Ä—ã, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è)
  * –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã –∏–ª–∏ framework
  * Actionable takeaway
  * –í–æ–ø—Ä–æ—Å –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è
- **–§–æ—Ä–º–∞—Ç**: –ö–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã (2-3 —Å—Ç—Ä–æ–∫–∏), bullet points –¥–ª—è —Å–ø–∏—Å–∫–æ–≤
- **–≠–º–æ–¥–∑–∏**: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ, —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (üìä üéØ ‚úÖ)
- **–•—ç—à—Ç–µ–≥–∏**: #ProductManagement #ProductStrategy #PMTips #ProductLeadership #ProductDevelopment
- **–ò–∑–±–µ–≥–∞—Ç—å**: –°–ª–∏—à–∫–æ–º –æ–±—â–∏—Ö —Å–æ–≤–µ—Ç–æ–≤, –ø—Ä–æ–¥–∞–∂–Ω–æ–≥–æ —Ç–æ–Ω–∞, buzzwords –±–µ–∑ —Å–º—ã—Å–ª–∞

–ü–†–ò–ú–ï–†–´ –•–û–†–û–®–ò–• –•–£–ö–û–í –î–õ–Ø PM:
‚ùå "–•–æ—Ç–∏—Ç–µ —É–ª—É—á—à–∏—Ç—å —Å–≤–æ–π –ø—Ä–æ–¥—É–∫—Ç?" (—Å–ª–∏—à–∫–æ–º –æ–±—â–æ)
‚úÖ "83% PM –Ω–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç —ç—Ç—É –º–µ—Ç—Ä–∏–∫—É. –ê –æ–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç churn." (–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ + –∏–Ω—Ç—Ä–∏–≥–∞)

‚ùå "5 —Å–æ–≤–µ—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–¥–∞–∫—Ç –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤" (–±–∞–Ω–∞–ª—å–Ω–æ)
‚úÖ "–ú—ã –ø–æ—Ç—Ä–∞—Ç–∏–ª–∏ 3 –º–µ—Å—è—Ü–∞ –Ω–∞ feature, –∫–æ—Ç–æ—Ä—É—é –Ω–∏–∫—Ç–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç. –ß—Ç–æ –º—ã —É–ø—É—Å—Ç–∏–ª–∏?" (–∏—Å—Ç–æ—Ä–∏—è + –±–æ–ª—å)

–í–ê–ñ–ù–û: 
- –§–æ–∫—É—Å –Ω–∞ –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï —Å–æ–≤–µ—Ç—ã, –Ω–µ —Ç–µ–æ—Ä–∏—é
- –ò—Å–ø–æ–ª—å–∑—É–π —Ä–µ–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã –∏ –¥–∞–Ω–Ω—ã–µ
- –ì–æ–≤–æ—Ä–∏ –Ω–∞ —è–∑—ã–∫–µ PM (discovery, backlog, stakeholders, churn, activation)
- –í–°–ï–ì–î–ê –Ω–∞—á–∏–Ω–∞–π —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ product-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤!"""

    response = self.client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=4096,
        system=system_prompt,
        tools=self.tools,
        messages=self.conversation_history
    )

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ø–æ—á–∫—É –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (tool_use/tool_result)
    while response.stop_reason == "tool_use":
        tool_use_blocks = [block for block in response.content if getattr(block, "type", None) == "tool_use"]
        for tool_use_block in tool_use_blocks:
            tool_name = tool_use_block.name
            tool_input = tool_use_block.input

            print(f"\nüîß {tool_name}")
            print(f"üìù {json.dumps(tool_input, ensure_ascii=False, indent=2)}")

            tool_result = self.process_tool_call(tool_name, tool_input)

            print(f"‚úÖ {json.dumps(tool_result, ensure_ascii=False, indent=2)[:200]}...")

            # –í—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–≥–æ –ø–∞—Ä–æ–π: assistant (tool_use), user (tool_result)
            self.conversation_history.append({
                "role": "assistant",
                "content": [tool_use_block]
            })
            self.conversation_history.append({
                "role": "user",
                "content": [{
                    "type": "tool_result",
                    "tool_use_id": tool_use_block.id,
                    "content": json.dumps(tool_result, ensure_ascii=False)
                }]
            })

            # –ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã tool_use/tool_result
            response = self.client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=4096,
                system=system_prompt,
                tools=self.tools,
                messages=self.conversation_history
            )
            if response.stop_reason != "tool_use":
                break  # –µ—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤–æ–≥–æ tool_use, –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç
    final_response = ""
    for block in response.content:
        if hasattr(block, "text"):
            final_response += block.text

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (–º–æ–∂–Ω–æ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å, –µ—Å–ª–∏ history –Ω—É–∂–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—á–µ—Ä–µ–¥–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤)
    self.conversation_history.append({
        "role": "assistant",
        "content": response.content
    })

    return final_response
